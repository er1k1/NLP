{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import pdfminer\n",
    "from pdfminer.pdfparser import PDFSyntaxError\n",
    "import os\n",
    "import re\n",
    "ROOT_URL = \"https://www.drugs.com/answers/questions/\"\n",
    "\n",
    "\n",
    "def page_soup(url):\n",
    "    \"\"\"return a soup object\"\"\"\n",
    "    html = requests.get(url).text\n",
    "    return bs(html, 'html.parser')\n",
    "\n",
    "\n",
    "def get_pdf_hrefs(url):\n",
    "    pdf_href_list = []  # {page_num: [BULAs URIs]}\n",
    "    soup = page_soup(url)\n",
    "\n",
    "    pdf_link_list = soup.findAll('a')\n",
    "    for link in pdf_link_list:\n",
    "        if re.search('/answers/[^/]*\\.html',link.attrs['href']):\n",
    "\n",
    "            pdf_href_list.append(link.attrs['href'])\n",
    "    return pdf_href_list\n",
    "\n",
    "def dump_list_to_json_file(list_, file_name):\n",
    "    with open(file_name, 'w') as fp:\n",
    "        json.dump(list_, fp, indent=4)\n",
    "\n",
    "\n",
    "for a in range(0,9):\n",
    "    href_list = []\n",
    "    for b in range(1 + a*819, 819 + a*819):\n",
    "        query = '?page=' + str(b)\n",
    "        href_list += get_pdf_hrefs(ROOT_URL + query)\n",
    "    file_name = 'hreflist' + str(a) + '.json'\n",
    "    dump_list_to_json_file(href_list, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_list_from_json(filename):\n",
    "    with open(filename) as infile:\n",
    "        j = json.load(infile)\n",
    "    return j\n",
    "\n",
    "def get_sentences(url):\n",
    "    base = \"https://www.drugs.com\"\n",
    "    soup = page_soup(base + url)\n",
    "    pat = re.compile('.*\\(([0-9]+)\\).*')\n",
    "    resphead = soup.find('div', attrs={'class':'replyListHeader'})\n",
    "    mat =re.match(pat, resphead.text.strip('\\n'))\n",
    "    postcontents = soup.find('div', attrs={'class':'postContent'})\n",
    "    if postcontents:\n",
    "        responses = [postcontents.text.replace('\\n','')]\n",
    "        if mat:\n",
    "            num = int(mat.group(1))\n",
    "            listcontents = soup.findAll('div', attrs={'class':'listContent'})\n",
    "            responses += [listcontents[x].text.replace('\\n','').strip('Votes:[+\\-0-9] Comment Vote up Report') for x in range(1,num + 1)]\n",
    "        return responses\n",
    "    else:\n",
    "        return[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16155\n",
      "17153\n",
      "16298\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "15794\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "15766\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "15784\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "15165\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n",
      "An exception occurred: list index out of range\n"
     ]
    }
   ],
   "source": [
    "for a in range(2,9):\n",
    "    file_name = 'hreflist' + str(a) + '.json'\n",
    "    sfile_name = 'sentencelist' + str(a) + '.json'\n",
    "    \n",
    "    href_list = read_list_from_json(file_name)\n",
    "    print(len(href_list))\n",
    "    sentencelist = []\n",
    "    for url in href_list:\n",
    "        try:\n",
    "            sentencelist += get_sentences(url)\n",
    "        except BaseException as error:\n",
    "            print('An exception occurred: {}'.format(error))\n",
    "            continue\n",
    "    dump_list_to_json_file(sentencelist, sfile_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def removepuncandsplit(sentence):\n",
    "    p=string.punctuation\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "\n",
    "    for x in p:\n",
    "        sentence = sentence.replace(x,'') \n",
    "    word_list = sentence.split(' ')\n",
    "    filtered_words = [word for word in word_list if word not in stopWords]\n",
    "    return filtered_words\n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "def sentencetokenize(text):\n",
    "    punkt_param = PunktParameters()\n",
    "    punkt_param.abbrev_types = set(['dr', 'vs', 'mr', 'mrs', 'prof', 'inc'])\n",
    "    sentence_splitter = PunktSentenceTokenizer(punkt_param)\n",
    "    sentences = sentence_splitter.tokenize(text)\n",
    "    tokdsentences = []\n",
    "    for sentence in sentences:\n",
    "        tokdsentences.append(removepuncandsplit(sentence))\n",
    "    return tokdsentences\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n"
     ]
    }
   ],
   "source": [
    "sentencelist = []\n",
    "for a in range(0,9):\n",
    "    sfile_name = 'sentencelist' + str(a) + '.json'\n",
    "    \n",
    "    sentencelist += read_list_from_json(sfile_name)\n",
    "\n",
    "sentences = []\n",
    "for i,sentence in enumerate(sentencelist):\n",
    "    if i%10000 == 0:\n",
    "        print(i)\n",
    "    sentences += sentencetokenize(sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "min_count = 5\n",
    "size = 50\n",
    "window = 5\n",
    " \n",
    "model = Word2Vec(sentences, min_count=min_count, size=size, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646221"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fats', 0.8851694464683533),\n",
       " ('carbohydrates', 0.8768407106399536),\n",
       " ('fibre', 0.8678574562072754),\n",
       " ('protein', 0.8522319793701172),\n",
       " ('carb', 0.8521303534507751),\n",
       " ('saturated', 0.8519685864448547),\n",
       " ('caloric', 0.8446818590164185),\n",
       " ('nutritious', 0.8128793239593506),\n",
       " ('salads', 0.8053873181343079),\n",
       " ('refined', 0.8044947385787964)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('carbohydrate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = open('faqmodel.pkl', 'wb')\n",
    "pickle.dump(model, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49494"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-534d586a17cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'faqmodel.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pickle'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\apps\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1118\u001b[0m             \u001b[0mWord2VecKeyedVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apps\\Anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\apps\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: unknown encoding: pickle"
     ]
    }
   ],
   "source": [
    "KeyedVectors.save('faqmodel.pkl',binary=True,encoding='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('faqmodel.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
